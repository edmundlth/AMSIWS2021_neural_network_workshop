{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Credits\n",
    "This borrows, at times verbatim, from various notebooks/blogs on the internet which themselves borrow heavily from the [official PyTorch tutorials](https://github.com/pytorch/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "Construct a 5x3 matrix, uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY\n",
    "Use official PyTorch documentation to perform the following\n",
    "+ Make a tensor of size (2, 17)\n",
    "+ Make a torch.FloatTensor of size (3, 1)\n",
    "+ Make a torch.LongTensor of size (5, 2, 1) then fill it all with 7s\n",
    "+ Make a torch.ByteTensor of size (5,) and then make it [0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "We can perform addition operation in quite a few different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY\n",
    "+ multiply two tensors\n",
    "+ multiply two tensors, but inplace\n",
    "+ division of two tensors\n",
    "+ matrix multiplication of two tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "Sometimes we'll need to convert torch tensor to numpy array or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b=torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY\n",
    "+ create torch tensor of size (5,2) containing all ones\n",
    "+ convert it to numpy\n",
    "+ convert it back to torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "The ``autograd`` package plays a central role in PyTorch. Let's consider the simplest neural network, with input ``x``, parameters ``w`` and ``b``, and some loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, ``w`` and ``b`` are **parameters**, which we need to optimize. Thus, we need to be able to compute the gradients of loss\n",
    "function with respect to those variables. In order to do that, we set the ``requires_grad`` property of those tensors as you can see above.\n",
    "\n",
    "The backward propagation is stored in ``grad_fn`` property of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to\n",
    "compute the derivatives of our loss function with respect to parameters,\n",
    "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
    "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of\n",
    "``x`` and ``y``. To compute those derivatives, we call\n",
    "``loss.backward()``, and then retrieve the values from ``w.grad`` and ``b.grad``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all tensors with ``requires_grad=True`` are tracking their\n",
    "computational history and support gradient computation. However, there\n",
    "are some cases when we do not need to do that, for example, when we have\n",
    "trained the model and just want to apply it to some input data, i.e. we\n",
    "only want to do *forward* computations through the network. We can stop\n",
    "tracking computations by surrounding our computation code with\n",
    "``torch.no_grad()`` block or alternatively use ``detach``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward on half moons\n",
    "\n",
    "Run the following to generate the half moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 500\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(1000, noise=0.20, random_state=1234)\n",
    "\n",
    "# define train, validation, and test sets\n",
    "X_tr = X[:num_samples].astype('float32')\n",
    "X_val = X[num_samples:(num_samples+100)].astype('float32')\n",
    "X_te = X[(num_samples+100):].astype('float32')\n",
    "\n",
    "# and labels\n",
    "y_tr = y[:num_samples].astype('int32')\n",
    "y_val = y[num_samples:(num_samples+100)].astype('int32')\n",
    "y_te = y[(num_samples+100):].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.Spectral)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "num_features = X_tr.shape[-1]\n",
    "num_output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def show_separation(net):\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    xx, yy = np.mgrid[-1.5:2.5:.01, -1.:1.5:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    batch = torch.from_numpy(grid).type(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.max(F.softmax(net(batch),dim=1),dim=1).values\n",
    "        probs = probs.numpy().reshape(xx.shape)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_title(\"Decision boundary\", fontsize=14)\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                          vmin=0, vmax=1)\n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    ax.scatter(X_te[:,0], X_te[:, 1], c=y_te, s=50,\n",
    "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "               edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "    ax.set(xlabel=\"test $X_1$\", ylabel=\"test $X_2$\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through fitting logistic regression (which is an MLP with zero hidden layers) to this (using SGD). Afterwards you'll be asked to upgrade the logistic regression model to an MLP with at least one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Setting up variables, these variables are weights in your \n",
    "        # network that can be updated while running our graph.\n",
    "        # Notice, to make a hidden layer, the weights need to have the \n",
    "        # following dimensionality:\n",
    "        #   W[number_of_units_going_out, number_of_units_going_in]\n",
    "        #   b[number_of_units_going_out]\n",
    "        # in the example below we have 2 input units (num_features) and 2 output units (num_output)\n",
    "        # so our weights become W[2, 2], b[2]\n",
    "        # if we want to make a hidden layer with 100 units, we need to define the shape of the\n",
    "        # first weight to W[100, 2], b[2] and the shape of the second weight to W[2, 100], b[2]\n",
    "        \n",
    "        self.W_1 = Parameter(torch.randn(num_output, num_features)) \n",
    "        self.b_1 = Parameter(torch.randn(num_output))\n",
    "        \n",
    "#         hidden layer\n",
    "#         self.W_2 = Parameter(torch.randn(num_output, 100)) \n",
    "#         self.b_2 = Parameter(torch.randn(num_output))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Read documentation on F.linear to confirm this does what we want it to do\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.linear(x, self.W_2, self.b_2)\n",
    "\n",
    "        return x # later, the activation function softmax is to be performed on the second dimension \n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's sometimes helpful to print a few things associated to ``net``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all parameters in your network\n",
    "print(\"NAMED PARAMETERS\")\n",
    "print(list(net.named_parameters()))\n",
    "print()\n",
    "# the .parameters() method simply gives the Tensors in the list\n",
    "print(\"PARAMETERS\")\n",
    "print(list(net.parameters()))\n",
    "print()\n",
    "\n",
    "# list individual parameters by name\n",
    "print('WEIGHTS')\n",
    "print(net.W_1)\n",
    "print(net.W_1.size())\n",
    "print('\\nBIAS')\n",
    "print(net.b_1)\n",
    "print(net.b_1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we understand better what ``Parameter`` is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = net.W_1\n",
    "print(\"## this is the tensor\")\n",
    "print(param.data)\n",
    "print(\"\\n## this is the tensor's gradient\")\n",
    "print(param.grad)\n",
    "# notice, the gradient is undefined because we have not yet run a backward pass\n",
    "\n",
    "print(\"\\n## is it a leaf in the graph?\")\n",
    "print(param.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the cross entropy loss function from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(ys, ts):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -torch.sum(ts * torch.log(ys), dim=1, keepdim=False)\n",
    "    # averaging over samples\n",
    "    return torch.mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two helper functions, one for calculating accuracy and the other that converts outputs to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], torch.max(ts, 1)[1])\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network we will use (batch) gradient descent. Minibatch SGD has many appealing properties compared to batch gradient descent. I think it's a bit easier to work with minibatches through ``DataLoader`` structure in PyTorch. So until we introduce that, let's just keep it simple with batch gradient descent. \n",
    "\n",
    "We can still use the SGD option in ``torch.optim`` to take care of the optimization which can handle batches that are 1) the entire dataset, 2) one data point at a time, 3) minibatches of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get to the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training passses\n",
    "num_epochs = 1000\n",
    "# store loss and accuracy for information\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "inputs = torch.from_numpy(X_tr)\n",
    "targets = torch.from_numpy(onehot(y_tr, num_output)).float()\n",
    "\n",
    "# training loop\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    # zero out accumulated gradients in parameters\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass\n",
    "    output = net(inputs)\n",
    "    prob = F.softmax(output, dim=1)\n",
    "    # compute cross entropy loss\n",
    "    loss = cross_entropy(prob, targets)\n",
    "    # compute gradients given loss\n",
    "    loss.backward()\n",
    "    # update the parameters given the computed gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # the main training loop finishes above, below let's do some logging.\n",
    "    \n",
    "    # store training loss\n",
    "    train_acc = accuracy(output, targets)\n",
    "    train_losses.append(loss.data.numpy())\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # get validation inputs and expected output as torch Variables and make sure type is correct\n",
    "    val_inputs = torch.from_numpy(X_val)\n",
    "    val_targets = torch.from_numpy(onehot(y_val, num_output)).float()\n",
    "    \n",
    "    # predict with validation inputs\n",
    "    with torch.no_grad():\n",
    "        val_output = net(val_inputs)\n",
    "        # compute loss and accuracy\n",
    "        prob = F.softmax(val_output, dim=1)\n",
    "        val_loss = cross_entropy(prob, val_targets)\n",
    "        val_acc = accuracy(val_output, val_targets)\n",
    "    \n",
    "    # store loss and accuracy\n",
    "    val_losses.append(val_loss.data.numpy())\n",
    "    val_accs.append(val_acc.data.numpy())\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %i, \"\n",
    "              \"Train Cost: %0.3f\"\n",
    "              \"\\tVal Cost: %0.3f\"\n",
    "              \"\\t Val acc: %0.3f\" % (e, \n",
    "                                     train_losses[-1],\n",
    "                                     val_losses[-1],\n",
    "                                     val_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation metrics printed above might guide you in your hyperparameter search. Instead of reading the chart above, we can also graph it for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "epoch = np.arange(len(train_losses))\n",
    "plt.plot(epoch, train_losses, 'r', label='Train Loss')\n",
    "plt.plot(epoch, val_losses, 'b', label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accs, 'r', label='Train Acc')\n",
    "plt.plot(epoch, val_accs, 'b', label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we correctly put the test set into a vault. Now that we are done training, we can get the test set out of the vault and see how we really did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test input and expected output\n",
    "test_inputs = torch.from_numpy(X_te)\n",
    "test_targets = torch.from_numpy(onehot(y_te, num_output)).float()\n",
    "# predict on testset\n",
    "te_output = net(test_inputs)\n",
    "prob = F.softmax(te_output, dim=1)\n",
    "# compute loss and accuracy\n",
    "te_loss = cross_entropy(prob, test_targets)\n",
    "te_acc = accuracy(te_output, test_targets)\n",
    "print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\" % (te_loss.data.numpy(), te_acc.data.numpy()))\n",
    "\n",
    "show_separation(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY\n",
    "+ We used the softmax activation function in the output layer which takes a vector input and spits back a probability vector of the same dimension. The sigmoid activation function is also frequently used in the output layer. See if you can figure out what needs to be modified to run the code above with sigmoid output activation function. \n",
    "+ Create a more expressive neural network than logistic regression by inserting at least one more hidden layer.\n",
    "+ In the hidden layer, experiment with more or less hidden units\n",
    "+ As you increase the number of hidden layers or hidden units per layer, do you observe any overfitting?\n",
    "+ Read up on the ``torch.optim`` package and upgrade SGD to momentum or adam.\n",
    "+ Try minibatch SGD by using ``DataLoader``. Starter code is provided below.\n",
    "+ What's the highest accuracy you can get on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "training_set = TensorDataset(inputs, targets)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=50)\n",
    "\n",
    "val_set = TensorDataset(val_inputs, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=50)\n",
    "\n",
    "test_set = TensorDataset(test_inputs, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50)\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "train_losses = []\n",
    "for e in range(num_epochs):  # <----------- iterate over the dataset several times\n",
    "    for x_batch, y_batch in train_loader:  # <------ iterate over the dataset. Since we use SGD and not GD, we take batches of a given size\n",
    "        optimizer.zero_grad()  # <------------- zero out the gradients of the model\n",
    "        output = net(x_batch)  # <------------- get \"logits\" from the model\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        loss = cross_entropy(prob, y_batch)  # <--- calculate \"loss\" for logistic regression\n",
    "        loss.backward()  # <------------------- calculate gradients\n",
    "        optimizer.step()  \n",
    "        train_losses.append(loss.detach())\n",
    "\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses)\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
