{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa730c87-5a54-4066-a86e-9015c8bff93a",
   "metadata": {},
   "source": [
    "# Deep Probabalistic Models: Tutorial 1\n",
    "## Tutorial 1: Flow-Based Models and GAN\n",
    "\n",
    "Welcome to the first tutorial for this week! This tutorial is a brief computational *exploration* of flow-based models and GANs within PyTorch. \n",
    "\n",
    "The goal of this tutorial is simple: for you to have you play around with flows and GANs on some simple examples (the full Iris data, and a swiss roll dataset).\n",
    "\n",
    "This notebook has also been written in a manner that will serve as a nice reference implementation for you in the future. \n",
    "\n",
    "**N.B. Please be sure to run each code cell as your progress through the notebook.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b2b3a-978f-41aa-811b-a25975f55476",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Flow-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478dc01-f9c4-4e1f-87d8-4a83f68f8403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Guided Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba7b81-8307-48bd-81c7-24b106a442ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "torch.manual_seed(123)\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcd1ea-29ce-4339-a114-1716035574d7",
   "metadata": {},
   "source": [
    "The following code cell will import the famous \"Iris\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f5ec2-7dd1-4a1c-8a33-540ae5196e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "dataset = iris.data\n",
    "dataset = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "print(f\"Dataset Loaded. Observations: {dataset.shape[0]:.0f}, dimension: {dataset.shape[1]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f8958-5195-4198-9064-ff42bf211632",
   "metadata": {},
   "source": [
    "**Flows typically fit quicker if the data is first standardized** (remove the sample mean and divide by sample variance for each column), so we do that below using a `StandardScaler` from `sklearn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523532b2-f5b1-4ab6-8af0-c9d5cc099107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset = StandardScaler().fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ca9dc-b082-45b1-81ca-ae941d8fa1bb",
   "metadata": {},
   "source": [
    "Run the code below to visualize the data with four different bivariate plots (visualizing two dimensions at a time). You will notice that the data has an interesting shape and thus will require a flexible model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a75945-162c-4c62-8fe2-9cf49a286a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = [[0,1], [2,3], [0,3], [0,2]]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1, aspect='equal')\n",
    "    ids = iter[i]\n",
    "    plt.plot(dataset[:,ids[0]], dataset[:,ids[1]],'b.')\n",
    "    plt.xlabel(iris.feature_names[2])\n",
    "    plt.ylabel(iris.feature_names[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9cf50-7ff3-4f24-b91b-ddd316a33087",
   "metadata": {},
   "source": [
    "Next, we will fit a flow based model to the four-dimensional data (note that the figure in the lectures was only fit to the 2D data of petal length vs. petal width, so we are doing something a little different to that). \n",
    "\n",
    "The code below creates a 4D base distribution, as well as a single autoregressive transform with each of the four splines having 8 bins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bea59-9afb-4b09-924d-c8b06d9a2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "p = 4 \n",
    "\n",
    "# Specifies that Z ~ N(0,I)\n",
    "distZ = dist.Normal(torch.zeros(4), torch.ones(4))\n",
    "\n",
    "# Defines the Transformation \n",
    "T = pyro.distributions.transforms.spline_autoregressive(input_dim=p, hidden_dims=[20,20],\n",
    "                                                        count_bins=8)\n",
    "\n",
    "# Creates a distribution object which is Z transformed with the transformation T \n",
    "distX = dist.TransformedDistribution(distZ, [T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2158b42-6cb6-4a6c-807a-66af934a230e",
   "metadata": {},
   "source": [
    "The code below computes the total number of parameters used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11646d-5342-4e1e-858a-c3e8d83a76bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_count = 0 \n",
    "for param in T.parameters():\n",
    "    param_count += len(param)\n",
    "    \n",
    "print(\"Total Parameters\", param_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47552bc-67bd-4d4e-8c32-dad1840d4fa7",
   "metadata": {},
   "source": [
    "That is a **lot** of parameters for fitting a 4D density (by contrast, you can fit a mixture of two Gaussians with only 16!), but such is the nature of using neural networks. \n",
    "\n",
    "The `train_flow_model` function below takes five inputs: \n",
    "\n",
    "* `dataset` : this is a dataset in the form of a rectangular arrar (`numpy` or `torch.Tensor`)\n",
    "* `params` : a collection of parameters which the optimizer will optimize over \n",
    "* `num_samples` : the number of subsampled observations from the data at each iteration\n",
    "* `steps` : the total number of steps that the optimizer will take \n",
    "* `lr`: the learning rate parameter of the optimizer. Note that when deeper neural nets or longer flows are used, this will need to be set smaller to ensure stable training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73358acc-67cd-4c24-a6a6-613c1c7f86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Flow Model           \n",
    "def train_flow_model(dataset, params, num_samples=32, steps = 2501, lr = 1e-3):            \n",
    "    dataset = torch.tensor(dataset, dtype=torch.float)\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    losslog = []\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if num_samples == 'all':\n",
    "            loss = -distX.log_prob(dataset).mean()\n",
    "        else: \n",
    "            ids = np.random.choice(dataset.shape[0], num_samples) # randomly choose subsampling indices with replacement\n",
    "            loss = -distX.log_prob(dataset[ids,:]).mean() # estimator of mean loss of training data (equivalent to using full log-likelihood in maximization)\n",
    "        \n",
    "        loss.backward()\n",
    "        losslog.append(loss.item())\n",
    "        optimizer.step()\n",
    "        distX.clear_cache()\n",
    "\n",
    "        if step % 250 == 0: print('step: {}, loss: {}'.format(step, loss.item()))\n",
    "    \n",
    "    return losslog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b1ccb-0683-4166-8bb5-757363bdb4da",
   "metadata": {},
   "source": [
    "The code below will train the flow we set up for `Xdist` previously. It may take a minute or so depending on the number of steps and subsample batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c55b1-b704-4b35-83b7-12e738e520f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losslog = train_flow_model(dataset, params = T.parameters(), steps=5001, num_samples=64)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518d020-84ff-4bf9-9a7b-d78114c87465",
   "metadata": {},
   "source": [
    "Plotting the (estimated) loss over time seems to imply that the training has converged.\n",
    "\n",
    "**Pro Tip:** If training is not performing well, or is unstable, try reducing the learning rate, and/or increasing the number of subsamples used at each iteration. The former is typically required for deeper (more transforms) flows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061877e0-eb89-407b-8871-5695ec41e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losslog)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337c7a9-d162-4d99-8d43-95f07516fa75",
   "metadata": {},
   "source": [
    "### Transformation Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7d6e6-7ed4-4759-afc2-9760e73be77e",
   "metadata": {},
   "source": [
    "Prior to looking at our fit, we will talk about how to use the transformation object we created that defines our distributional family. \n",
    "\n",
    "To create new data (or assess how well the original data gets transformed back to a normal), we wish to send samples through the learned $T$ and its inverse. \n",
    "\n",
    "Recall that we created a transformation object `T` above. \n",
    "\n",
    "Transformation objects have both a forward and inverse method defined on them. The following methods are worth noting: \n",
    "\n",
    "\n",
    "1. `T(Z)`: returns $T(Z)$\n",
    "1. `T.inv(X)`: returns $T^{-1}(X)$\n",
    "\n",
    "The above is demonstrated below by generating a sample $Z \\sim {\\cal N}(0,{\\rm I})$, computing $X = T(Z)$, and then computing $Z = T^{-1}(X)$. Note that putting the sample through the transform and then the resulting sample through the inverse transform correctly yields the original sample back. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13d25f-26ff-4013-bd4a-4316608930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = distZ.sample()\n",
    "X = T(Z)\n",
    "_Z = T.inv(X)\n",
    "print('Z:', Z, '\\nX = T(Z):', X, '\\nT.inv(X):', _Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750f471-b035-4594-a57f-e69c86462bf8",
   "metadata": {},
   "source": [
    "The above also works with an arbitrary amount of samples. The `.sample` method of the base distribution just needs to be passed the requested number of samples inside of `[ ]`. Calling `distZ.sample()` is the same as `distZ.sample([1])`. Below, we generate 3 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fb017-2f56-4d3b-b6c6-a9ef6354fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = distZ.sample([3])\n",
    "X = T(Z)\n",
    "_Z = T.inv(X)\n",
    "print('Z:', Z, '\\nX = T(Z):', X, '\\nT.inv(X):', _Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1653f9-3f69-430a-a9f1-d1eda9ca3655",
   "metadata": {},
   "source": [
    "Below, we sample 500 times from the learned distribution, and plot the observations in bivariate plots with the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b871b1f-55ee-4dbc-ab93-b4c2b9a5b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = distX.sample([500]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035203af-8230-4642-8dcf-8492e6b14926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iter = [[0,1], [2,3], [0,3], [0,2]]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1, aspect='equal')\n",
    "    ids = iter[i]\n",
    "    plt.plot(X[:,ids[0]], X[:,ids[1]],'r.', label = \"Flow\", alpha=0.15)\n",
    "    plt.plot(dataset[:,ids[0]], dataset[:,ids[1]],'b.', label = \"Data\", alpha=0.35)\n",
    "    plt.xlabel(iris.feature_names[2])\n",
    "    plt.ylabel(iris.feature_names[3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf2bf0-85c4-4cd2-a260-382b3e086b35",
   "metadata": {},
   "source": [
    "Note that, as is the case with minimizing $KL(p||q)$, the learned distribution is very conservative where it places mass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecf631-8880-47f8-bc03-abbcce227672",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(dataset, dtype=torch.float)\n",
    "dataZ = T.inv(data_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21e1bd-9371-4208-8f0e-5ba9ee2df066",
   "metadata": {},
   "source": [
    "If the model has fit the training data well, we should expect all the bivariate plots of the **inverse transformed** data to look like samples from a $N(\\mathbf{0}, I)$ distribution. Below we plot the inverse transformed dataset, along with samples from a $N(\\mathbf{0}, I)$ for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f374a-893f-4e8a-991b-33bab9582ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "realZ = np.random.randn(500,4)\n",
    "\n",
    "iter = [[0,1], [2,3], [0,3], [0,2]]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1, aspect='equal')\n",
    "    ids = iter[i]\n",
    "    plt.plot(realZ[:, 0], realZ[:,1], 'r.', alpha=0.25, label = '$Z \\sim N(0,I)$',)\n",
    "    plt.plot(dataZ[:, 0], dataZ[:,1], 'b.', alpha=1, label = '$T^{-1}(X)$')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f45f4c-7170-4109-b430-11de6a2f9f77",
   "metadata": {},
   "source": [
    "## Multiple Transformations\n",
    "\n",
    "For the next example, we will use multiple iterations of Real NVP with reverse permutation operations to fit data of a very challenging shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a1fee-3b3a-4033-a75f-b25f14f56713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(test_data[:, 0], test_data[:,1], 'b.', alpha=0.05)\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "import sklearn\n",
    "\n",
    "_data = make_circles(n_samples=1000, factor=.2, noise=.05)[0]\n",
    "_data_test = make_circles(n_samples=100, factor=.2, noise=.05)[0]\n",
    "\n",
    "L = np.array([[1,0],[.5,1]])\n",
    "data_circ = _data @ L.T\n",
    "data_circ_test = _data_test  @ L.T\n",
    "\n",
    "plt.plot(data_circ[:,0], data_circ[:,1], 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e6624-1fc5-48e6-b6e1-a3491c0a9c96",
   "metadata": {},
   "source": [
    "In this section, you will explore using flows of simpler transformations. The code below only does one Real NVP transformation, so it will leave half the variables unchanged as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b46fc8-63cf-4b8a-a33f-ab8106762b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "torch.manual_seed(123)\n",
    "\n",
    "p = 2\n",
    "distZ = dist.Normal(torch.zeros(p), torch.ones(p))\n",
    "\n",
    "rev_perm = torch.tensor(range(p-1,-1,-1))\n",
    "\n",
    "T=[]\n",
    "\n",
    "main_transform = pyro.distributions.transforms.affine_coupling \n",
    "\n",
    "T.append(main_transform(input_dim = p, hidden_dims=[25]))    \n",
    "\n",
    "\n",
    "n_layers = 1\n",
    "for i in range(n_layers-1): # does not do this is n_layers=0\n",
    "    T.append(dist.transforms.permute(input_dim=p, permutation=rev_perm))\n",
    "    T.append(main_transform(input_dim = p, hidden_dims=[25]))\n",
    "   \n",
    "params = []\n",
    "for tr in T: \n",
    "    if hasattr(tr, 'parameters'): params += list(tr.parameters())\n",
    "        \n",
    "distX = dist.TransformedDistribution(distZ, T)\n",
    "\n",
    "T = torch.distributions.ComposeTransform(T) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5174405-a81d-4329-855c-8aaf64c07e6a",
   "metadata": {},
   "source": [
    "Note that training using Real NVP without splines is very fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3589e-107b-4166-882d-342923409820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Flow Model \n",
    "\n",
    "data = torch.tensor(data_circ, dtype=torch.float)\n",
    "losslog = train_flow_model(dataset= data, params = params, num_samples=256, steps=10000, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bbcf9-0de8-4227-a09d-7fd66f2c99b3",
   "metadata": {},
   "source": [
    "Run the cell below to plot the result.Recall that the variable on the $x$-axis is marginally ${\\cal N}(0,1)$ as we only used one layer of Real NVP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b68576-f16f-4607-ae51-567ae2522cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losslog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7f1b8-56ab-4c08-a76d-05bd06fb8e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = distX.sample([data_circ.shape[0]]).detach().numpy()\n",
    "plt.plot(data_circ[:,0], data_circ[:,1], 'b.')\n",
    "plt.plot(X[:,0], X[:,1], 'r.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235709b-c5f0-4162-b854-d9ae6243a8c4",
   "metadata": {},
   "source": [
    "### Flow-Based Models: Exercise (Practical)\n",
    "\n",
    "Modify the Real NVP flow code above and train it so you obtain a good fit to the points. \n",
    "\n",
    "You may wish to change:\n",
    "\n",
    "* The **number of transforms** (layers) \n",
    "* The **number of hidden units** in the nueral nets (i.e., instead of [25], maybe [100]) in each layer. \n",
    "* The **number of hidden layers** in the neural nets (e.g., use [25,25,25] to have three hidden layers)\n",
    "* A **different transform from Real NVP** (e.g., `pyro.distributions.affine_autoregressive` - but for a challenge, no splines are allowed!)\n",
    "* The **learning rate** so training is more stable (this may necessitate more steps)\n",
    "* The **random see**d (maybe after changing the above you have a good model and just got unlucky if you didn't get a good fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dfdce-2b9a-4fa7-8e2d-34e57681266f",
   "metadata": {},
   "source": [
    "### Flow-Based Models: Exercise (Analytical) \n",
    "\n",
    " **Johnson's SU-distribution** is a four-parameter one-dimensional distribution arising from the transformation of a standard normal:\n",
    "\n",
    "$$X = \\mu + \\sigma \\sinh\\left(\\frac{Z - \\gamma}{\\delta} \\right)$$\n",
    "\n",
    "where $\\mu \\in \\mathbb{R}$, $\\gamma \\in \\mathbb{R}$, $\\sigma > 0$, $\\delta >0$, and $Z \\sim {\\cal N}(0,1)$. \n",
    "\n",
    "Using that $\\sinh^{-1}(x) = \\log\\left(x + \\sqrt{x^2+1}\\right)$, derive the probability density function of $X$ using the change of variables theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adb6cc-5fdf-4253-859a-7421c37f3820",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962beb31-92ba-4008-a7b0-05ee096d42eb",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will try to fit data from the **swiss roll** dataset using a GAN. As this dataset seems to follow the **manifold hypothesis** very well (data is a small perturbation from a 2D manifold in this case), we will try to fit it using a GAN which only has a dimension of two (2). We plot the same dataset viewed from two different angles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea7159-760d-479e-8b2f-ede277c68a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "def plot3d(data):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    ax = plt.subplot(1,2,1, projection='3d')\n",
    "    ax.scatter(data[:, 0], data[:, 1], data[:, 2], alpha = 0.15)\n",
    "    ax.view_init(5, -70)\n",
    "\n",
    "    ax = plt.subplot(1,2,2, projection='3d')\n",
    "    ax.scatter(data[:, 0], data[:, 1], data[:, 2], alpha = 0.15)\n",
    "    ax.view_init(10, -100)\n",
    "\n",
    "np.random.seed(123)\n",
    "data, clr = make_swiss_roll(5000, random_state=100, noise = 0.3)\n",
    "\n",
    "data = StandardScaler().fit_transform(data)\n",
    "plot3d(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75c1bf-5fb5-4628-9171-2c2b73467ad9",
   "metadata": {},
   "source": [
    "The class `manyGAN` implemented below contains implementations of GAN, NS-GAN, and Least Squares GANs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05736054-36a2-41c1-a6fb-58daed40abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "class manyGAN():\n",
    "    def __init__(self, data, dimZ, n_hidden=50, GANtype = \"GAN\"):\n",
    "        data_dim  = data.shape[1]\n",
    "        self.dimZ = dimZ\n",
    "        self.data = t.tensor(data, dtype=t.float) \n",
    "        self.n = data.shape[0]\n",
    "        self.type = GANtype\n",
    "        \n",
    "        print(\"Latent Dimension\", dimZ)\n",
    "        print(\"Data Dimension:\", data_dim)\n",
    "        \n",
    "        self.g = nn.Sequential(nn.Linear(dimZ, n_hidden), nn.ReLU(),\n",
    "                               nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "                               nn.Linear(n_hidden,data_dim))\n",
    "        \n",
    "        \n",
    "        d_parts = [nn.Linear(data_dim, n_hidden),nn.ReLU(),\n",
    "                   nn.Linear(n_hidden, n_hidden), nn.ReLU(),nn.Linear(n_hidden,1)]\n",
    "    \n",
    "        if GANtype in [\"GAN\", \"NSGAN\"]: d_parts.append(nn.Sigmoid())                       \n",
    "        \n",
    "        self.d = nn.Sequential(*d_parts) \n",
    "       \n",
    "    def train_GAN(self, n_steps=20001,n_samples = 256, d_steps=1, lrs = [1e-4,2e-4], seed=1): \n",
    "        torch.manual_seed(seed)\n",
    "        data = self.data\n",
    "        \n",
    "        self.opt_g = torch.optim.Adam(self.g.parameters(), lr=lrs[0])  \n",
    "        self.opt_d = torch.optim.Adam(self.d.parameters(), lr=lrs[1])  \n",
    "        \n",
    "        self.glog = []\n",
    "        self.dlog = []\n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            # ------------ train discriminator ---------------\n",
    "            for j in range(d_steps):\n",
    "                self.opt_d.zero_grad()\n",
    "                X = self.g(t.randn(n_samples, self.dimZ) )\n",
    "                ids = np.random.choice(data.shape[0],n_samples)\n",
    "                if self.type in [\"GAN\", \"NSGAN\"]:\n",
    "                    d_loss = -(t.mean(t.log(self.d(data))) + t.mean(t.log(1 - self.d(X))))\n",
    "                elif self.type == \"LSGAN\":\n",
    "                    d_loss = (t.mean((self.d(self.data)-1)**2) + t.mean(self.d(X)**2))/2\n",
    "                          \n",
    "                d_loss.backward() \n",
    "                self.dlog.append(d_loss.item())\n",
    "                self.opt_d.step()\n",
    "                    \n",
    "            # ----------- train generator ------------------\n",
    "            self.opt_g.zero_grad()\n",
    "            X = self.g(t.randn(n_samples, self.dimZ))\n",
    "\n",
    "            if self.type == \"GAN\":\n",
    "                g_loss = t.mean(t.log(1-self.d(X)))\n",
    "            elif self.type == \"NSGAN\":\n",
    "                g_loss = -t.mean(t.log(self.d(X)))  \n",
    "            elif self.type == \"LSGAN\":\n",
    "                g_loss = t.mean((self.d(X)-1)**2)/2\n",
    "                   \n",
    "            g_loss.backward() \n",
    "            self.glog.append(g_loss.item())\n",
    "            self.opt_g.step()\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print('iter: {}, generator loss: {:.2e}, \\\n",
    "                      discriminator loss: {:.2e}'.format(i, g_loss.item(), d_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25c6b3-a080-4e5e-8f4e-686e9666a138",
   "metadata": {},
   "source": [
    "The Code Below creates a new `manyGAN` object, and runs the training procedure. Note that you have many parameters to tweak in service of achieving your task of a good fit! (Rob's note: it is indeed possible, it took me a while but I did it). \n",
    "\n",
    "* `n_hidden` decides the number of hidden neurons in the (two) hidden layers of the generator and discriminator\n",
    "* `GANtype` determines which GAN loss to use, use either `GAN`, `NSGAN` or `LSGAN`. \n",
    "* `n_steps` is the number of training steps\n",
    "* `n_samples` is the subsample size (note the dataset has 5000 observations in total)\n",
    "* `d_steps` is the number of steps spent training the discriminator\n",
    "* `lrs` is a list of the two learning rates (first one is generator, second is discriminator\n",
    "* `seed` is the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15308b26-f5d1-4e88-a789-eeda30443e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimZ = 2 # do not change this! \n",
    "myGAN = manyGAN(data=data, dimZ = dimZ, n_hidden=10, GANtype = \"LSGAN\")\n",
    "myGAN.train_GAN(n_steps=5001, n_samples = 100, d_steps=1, lrs = [1e-3,2e-3], seed = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34cdec-df34-42e6-9114-b964f15f7d0f",
   "metadata": {},
   "source": [
    "The following code simulates from your trained GAN and compares the data to the learned data (viewed from two angles for ease of comparison). Use it to assess how you go after tweaking and refining parameters, and keep going until you have a good fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298ac5e-af0c-4138-887f-df17cd6d42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GANdata = myGAN.g(t.randn([5000,dimZ])).detach().numpy()\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.clf()\n",
    "ax = plt.subplot(1,2,1, projection='3d')\n",
    "\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c = 'blue', alpha = 0.1)\n",
    "ax.scatter(GANdata[:, 0], GANdata[:, 1], GANdata[:, 2], c='red' , alpha = 0.1)\n",
    "ax.view_init(5, -70)\n",
    "\n",
    "ax = plt.subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c = 'blue', alpha = 0.1)\n",
    "ax.scatter(GANdata[:, 0], GANdata[:, 1], GANdata[:, 2], c = 'red' , alpha = 0.1)\n",
    "ax.view_init(10, -100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e806f1-b17d-4262-8bff-11eb38c06514",
   "metadata": {},
   "source": [
    "The code below plots **just** the GAN generated data (you may wish to use it if the above plot makes it difficult to see what is happening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb3d26-4759-4400-84c3-06799e3687f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax = plt.subplot(1,2,1, projection='3d')\n",
    "ax.scatter(GANdata[:, 0], GANdata[:, 1], GANdata[:, 2], c = 'red', alpha = 0.1)\n",
    "ax.view_init(5, -70)\n",
    "\n",
    "ax = plt.subplot(1,2,2, projection='3d')\n",
    "ax.scatter(GANdata[:, 0], GANdata[:, 1], GANdata[:, 2], c = 'red', alpha = 0.1)\n",
    "ax.view_init(10, -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b4407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
